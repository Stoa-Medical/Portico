-- Supabase seed script for development data
-- Load with: psql postgresql://postgres:postgres@localhost:54322/postgres -f seed.sql
-- For full seeding with runtime sessions, use: ./scripts/generate_seed_data.py

-- Clear existing data first
TRUNCATE TABLE runtime_sessions, steps, signals, agents RESTART IDENTITY CASCADE;

-- Insert sample agents
INSERT INTO agents (name, type, description, agent_state, agent_name, agent_type)
VALUES
  ('Hacker News Scraper', 'information', 'Scrapes Hacker News, saves the data, summarizes it with LLM, and appends the summary to the file', 'stable', 'hacker_news_scraper', 'content'),
  ('HL7 to FHIR Converter', 'integration', 'Converts HL7 ADT messages to FHIR Patient & Encounter resources', 'stable', 'hl7_to_fhir_agent', 'transform'),
  ('Daily Topic Research', 'scheduler', 'Fetches and summarises daily regulatory news', 'stable', 'topic_research_agent', 'information'),
  ('Coordinator Agent', 'orchestration', 'Routes requests and negotiates solutions', 'stable', 'coordinator_agent', 'orchestration'),
  ('Specialist Agent', 'knowledge', 'Provides domain mapping expertise', 'stable', 'specialist_agent', 'knowledge'),

-- Insert sample steps for each agent
INSERT INTO steps (agent_id, name, description, step_type, step_content)
VALUES
  -- Hacker News Scraper steps
  (1, 'Scrape Hacker News', 'Grabs information from news.ycombinator.com and identifies main topics', 'webscrape', 'https://news.ycombinator.com'),
  (1, 'Save News Data', 'Saves the scraped news data to a file', 'python', E'import json\nimport os\n\n# Get the scraped data from the previous step\nnews_data = source.get("data", source)\n\n# Define the output file path - check both globals and source\ncustom_output_path = None\n# In Python steps, the RUN signal\'s initial_data is accessible directly via a global variable \'data\'\nif \'data\' in globals() and isinstance(data, dict) and "custom_output_path" in data:\n    custom_output_path = data["custom_output_path"]\n\n# Check the source for custom_output_path as a fallback\nif not custom_output_path and isinstance(source, dict) and "custom_output_path" in source:\n    custom_output_path = source["custom_output_path"]\n\n# Use default if no custom path was found\noutput_file = custom_output_path if custom_output_path else "/tmp/hacker_news_data.json"\n\ntry:\n    # Create directory if it doesn\'t exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    \n    # Save the data to a file\n    with open(output_file, "w") as f:\n        json.dump(news_data, f, indent=2)\n    \n    result = {\n        "file_path": output_file,\n        "item_count": len(news_data) if isinstance(news_data, list) else 1,\n        "status": "success"\n    }\nexcept Exception as e:\n    result = {\n        "error": str(e),\n        "status": "error"\n    }'),
  (1, 'Summarize News', 'Generates a summary of the key updates from Hacker News', 'prompt', 'You are a news analyst specializing in technology trends.\n\nI have scraped the top stories from Hacker News. Please analyze the data and provide a concise summary of the key updates and trends. Focus on identifying the main topics, any emerging patterns, and highlight the most significant stories based on points and discussion activity.\n\nFormat your response as a brief executive summary that could be shared with a technology team.'),
  (1, 'Append Summary', 'Appends the generated summary to the news data file', 'python', E'import json\nimport os\nimport glob\n\n# Get the summary from the previous step - using the standardized format\nsummary = source.get("response", "No summary generated")\n\n# We need to find the file path, but we don\'t have access to previous_results\n# Instead, use a hardcoded path or look for recently created files\nfile_path = "/tmp/hacker_news_data.json"\n\n# Check if the file exists before proceeding\nif not os.path.exists(file_path):\n    # Try to find the most recent json file in /tmp\n    json_files = glob.glob("/tmp/hacker_news*.json")\n    if json_files:\n        # Sort by creation time and get the most recent one\n        file_path = max(json_files, key=os.path.getctime)\n\n# In Python steps, the RUN signal\'s initial_data is accessible directly via a global variable \'data\'\n# so we can check if data and custom_output_path are defined\ntry:\n    # Custom output path might be in \'data\' if available from signal\n    custom_output_path = None\n    if \'data\' in globals() and isinstance(data, dict) and "custom_output_path" in data:\n        custom_output_path = data["custom_output_path"]\n    \n    # Check the source for custom_output_path as a fallback\n    if not custom_output_path and isinstance(source, dict) and "custom_output_path" in source:\n        custom_output_path = source["custom_output_path"]\n\n    # Load the existing data\n    with open(file_path, "r") as f:\n        data = json.load(f)\n    \n    # Create a new file with both data and summary\n    output_file = custom_output_path if custom_output_path else "/tmp/hacker_news_report.json"\n    # Create directory if it doesn\'t exist\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    \n    with open(output_file, "w") as f:\n        json.dump({\n            "data": data,\n            "summary": summary\n        }, f, indent=2)\n    \n    result = {\n        "original_file": file_path,\n        "report_file": output_file,\n        "status": "success"\n    }\nexcept Exception as e:\n    result = {\n        "error": str(e),\n        "status": "error"\n    }'),
  -- HL7 to FHIR Converter steps
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), 'Parse ADT', 'Parses HL7 ADT A04 messages to JSON', 'python', E'from typing import Dict, Any, List, Optional, Union\nfrom dataclasses import dataclass\nfrom functools import reduce\nimport re\nfrom enum import Enum, auto\n\nclass ParseError(Enum):\n    INVALID_FORMAT = auto()\n    MISSING_SEGMENTS = auto()\n    INCOMPLETE_MESSAGE = auto()\n\n# Result type for error handling\nResult = Union[Dict[str, Any], ParseError]\n\n@dataclass(frozen=True)\nclass HL7Segment:\n    """Immutable representation of an HL7 segment"""\n    segment_id: str\n    fields: List[str]\n\n# Parse a single HL7 segment into a structured format\ndef parse_segment(segment_text: str) -> HL7Segment:\n    """Parse a single HL7 segment string into an immutable HL7Segment object"""\n    parts = segment_text.split("|") \n    return HL7Segment(segment_id=parts[0], fields=parts[1:])\n\n# Map a list of segments into a structured data model\ndef segments_to_dict(segments: List[HL7Segment]) -> Dict[str, Any]:\n    """Convert segments to a structured dictionary representation"""\n    def reducer(acc: Dict[str, Any], segment: HL7Segment) -> Dict[str, Any]:\n        # Common field mappings for demo purposes\n        field_map = {\n            "PID": {\n                "patient_id": 3,  # Patient ID is field 3 in PID\n                "name": 5,        # Patient name is field 5\n                "dob": 7,         # Date of birth is field 7\n                "gender": 8        # Gender is field 8\n            },\n            "PV1": {\n                "visit_number": 19,  # Visit Number\n                "admission_type": 4,  # Admission Type\n                "attending_doctor": 7 # Attending Doctor\n            },\n            "MSH": {\n                "sending_app": 3,     # Sending application\n                "message_type": 9     # Message type\n            }\n        }\n        \n        # Only process segments we know how to handle\n        if segment.segment_id not in field_map:\n            return acc\n            \n        # Extract fields based on our mapping\n        segment_data = {}\n        for field_name, field_index in field_map[segment.segment_id].items():\n            try:\n                segment_data[field_name] = segment.fields[field_index-1] if field_index <= len(segment.fields) else ""\n            except IndexError:\n                segment_data[field_name] = ""\n                \n        # Create a new dict with our result (immutable pattern)\n        return {**acc, segment.segment_id: segment_data}\n    \n    # Use reduce to build up the dictionary\n    return reduce(reducer, segments, {})\n\n# Main parsing function that returns a Result type\ndef parse_hl7_message(hl7_text: str) -> Result:\n    """Parse an HL7 message string into structured data or return an error"""\n    # Normalize line endings and split into segments\n    cleaned_text = hl7_text.replace("\\r\\n", "\\n").replace("\\r", "\\n")\n    segment_lines = [line for line in cleaned_text.split("\\n") if line.strip()]\n    \n    # Validation checks\n    if not segment_lines:\n        return ParseError.INCOMPLETE_MESSAGE\n        \n    if not segment_lines[0].startswith("MSH"):\n        return ParseError.INVALID_FORMAT\n    \n    # Parse each segment\n    segments = [parse_segment(line) for line in segment_lines]\n    \n    # Required segments for ADT messages\n    required_segments = ["MSH", "PID"]\n    segment_ids = [s.segment_id for s in segments]\n    for req in required_segments:\n        if req not in segment_ids:\n            return ParseError.MISSING_SEGMENTS\n    \n    # Convert to structured dictionary\n    result = segments_to_dict(segments)\n    \n    # Add metadata\n    result["meta"] = {\n        "segment_count": len(segments),\n        "raw_length": len(hl7_text),\n        "timestamp": "2025-05-08T01:38:16-07:00"  # For demo purposes\n    }\n    \n    return result\n\n# Main process function - this is the entry point\ndef process_message():\n    # Get the input message from the signal data\n    raw_hl7 = source.get("raw_hl7", "MSH|^~\\\\&|SENDING|RECEIVING|ADT^A04")\n    \n    # Parse the message\n    parsed_result = parse_hl7_message(raw_hl7)\n    \n    # Pattern match on the result type\n    if isinstance(parsed_result, ParseError):\n        result = {\n            "status": "error",\n            "error": str(parsed_result.name),\n            "message": f"Failed to parse HL7 message: {parsed_result.name}"\n        }\n    else:\n        result = {\n            "status": "success",\n            "data": parsed_result,\n            "message": "Successfully parsed HL7 message"\n        }\n    \n    return result\n\n# Run the processor\nresult = process_message()'),
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), 'Map to FHIR', 'Maps HL7 JSON to FHIR Patient & Encounter resources', 'prompt', 'You are a FHIR mapping assistant. Convert the provided HL7 JSON into FHIR Patient and Encounter resources.'),
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), 'Persist FHIR', 'Persists generated FHIR resources', 'python', E'from typing import Dict, Any, List, Optional, Union, TypedDict\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom enum import Enum, auto\nfrom functools import partial\nimport json\nimport uuid\n\n# Type definitions\nclass PersistError(Enum):\n    INVALID_DATA = auto()\n    STORAGE_ERROR = auto()\n    TRANSACTION_ERROR = auto()\n\n# Result type pattern for functional error handling\nResult = Union[Dict[str, Any], PersistError]\n\n# Typed dict for FHIR resources\nclass FHIRResource(TypedDict):\n    resourceType: str\n    id: str\n    meta: Dict[str, Any]\n\n@dataclass(frozen=True)\nclass StorageResult:\n    """Immutable container for storage operation results"""\n    resource_id: str\n    resource_type: str\n    storage_location: str\n    timestamp: str\n\n# Pure function to generate a FHIR-compliant UUID\ndef generate_fhir_id() -> str:\n    """Generate a FHIR-compliant UUID without side effects"""\n    return f"urn:uuid:{uuid.uuid4()}"\n\n# Add metadata to a FHIR resource\ndef add_metadata(resource: Dict[str, Any]) -> Dict[str, Any]:\n    """Pure function to add metadata to a FHIR resource"""\n    if not resource.get("meta"):\n        resource = {\n            **resource,\n            "meta": {\n                "versionId": "1",\n                "lastUpdated": datetime.now().isoformat(),\n                "source": "Portico",\n                "profile": [f"http://hl7.org/fhir/StructureDefinition/{resource.get(\'resourceType\')}"]\n            }\n        }\n    return resource\n\n# Validate a FHIR resource (minimal validation for demo)\ndef validate_fhir_resource(resource: Dict[str, Any]) -> Optional[str]:\n    """Pure function to validate a FHIR resource, returns error message or None"""\n    if not resource.get("resourceType"):\n        return "Missing resourceType"\n    \n    if not resource.get("id") and resource.get("resourceType") not in ["Bundle", "Parameters"]:\n        return "Missing id"\n    \n    # More validation would go here in a real implementation\n    return None\n\n# Mock storage function (would connect to DB in real implementation)\ndef store_resource(resource: Dict[str, Any], storage_type: str = "fhir_store") -> Result:\n    """Store a FHIR resource and return a Result"""\n    # Validate the resource first\n    validation_error = validate_fhir_resource(resource)\n    if validation_error:\n        return PersistError.INVALID_DATA\n        \n    # Ensure resource has an ID\n    if not resource.get("id"):\n        resource = {**resource, "id": generate_fhir_id().split(":")[-1]}\n    \n    # Add metadata if needed\n    resource = add_metadata(resource)\n    \n    # In a real implementation, this would save to a database or FHIR server\n    # For demo purposes, we just log what would have happened\n    print(f"[DEMO] Storing {resource[\'resourceType\']} resource with ID {resource[\'id\']}\\n")\n    \n    # Return a success result with storage details\n    return StorageResult(\n        resource_id=resource["id"],\n        resource_type=resource["resourceType"],\n        storage_location=f"/{storage_type}/{resource[\'resourceType\']}/{resource[\'id\']}",\n        timestamp=datetime.now().isoformat()\n    )\n\n# Process incoming FHIR data\ndef process_fhir_data():\n    # Get FHIR data from previous step\n    fhir_data = source.get("data", {})\n    \n    # Extract resources (check various possible formats)\n    patient = fhir_data.get("patient", {})\n    encounter = fhir_data.get("encounter", {})\n    \n    # Validate and store each resource\n    results = {}\n    \n    # Store patient resource if present\n    if patient:\n        patient_result = store_resource(patient)\n        if isinstance(patient_result, PersistError):\n            results["patient"] = {"status": "error", "error": patient_result.name}\n        else:\n            # Convert dataclass to dict for JSON serialization\n            results["patient"] = {"status": "success", **asdict(patient_result)}\n    \n    # Store encounter resource if present\n    if encounter:\n        encounter_result = store_resource(encounter)\n        if isinstance(encounter_result, PersistError):\n            results["encounter"] = {"status": "error", "error": encounter_result.name}\n        else:\n            results["encounter"] = {"status": "success", **asdict(encounter_result)}\n    \n    # Return overall results\n    return {\n        "status": "success" if results else "error",\n        "message": f"Processed {len(results)} FHIR resources",\n        "resources": results\n    }\n\n# Execute the processing function\nresult = process_fhir_data()'),
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), 'Emit Finished', 'Emits an FYI signal with stored IDs', 'python', E'from typing import Dict, Any, List, Optional, Union, Protocol, Callable\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nimport json\nimport uuid\nfrom functools import reduce\n\n# Type definitions for functional design\nclass EmitError(Enum):\n    INVALID_PAYLOAD = auto()\n    SIGNAL_CREATION_FAILED = auto()\n    MISSING_RESOURCES = auto()\n\n# Protocol for dependency injection of the signal creation function\nclass SignalEmitter(Protocol):\n    def __call__(self, signal_type: str, agent_id: Optional[str], payload: Dict[str, Any]) -> Dict[str, Any]: ...\n\n# Result type pattern for error handling\nResult = Union[Dict[str, Any], EmitError]\n\n@dataclass(frozen=True)\nclass ResourceReference:\n    """Immutable reference to a stored FHIR resource"""\n    resource_type: str\n    resource_id: str\n    storage_path: str\n\n@dataclass(frozen=True)\nclass EmitResult:\n    """Immutable container for signal emission results"""\n    signal_id: str\n    timestamp: str\n    resource_count: int\n    resources: List[ResourceReference] = field(default_factory=list)\n\n# Pure function to create a resource reference from storage data\ndef create_resource_reference(resource_data: Dict[str, Any]) -> Optional[ResourceReference]:\n    """Extract resource reference data from a success response"""\n    # Skip resources with errors\n    if resource_data.get("status") != "success":\n        return None\n        \n    return ResourceReference(\n        resource_type=resource_data.get("resource_type", "unknown"),\n        resource_id=resource_data.get("resource_id", str(uuid.uuid4())),\n        storage_path=resource_data.get("storage_location", "")\n    )\n\n# Pure function to validate payload before emission\ndef validate_payload(payload: Dict[str, Any]) -> Optional[EmitError]:\n    """Validate signal payload before emission"""\n    if not payload:\n        return EmitError.INVALID_PAYLOAD\n        \n    if not payload.get("resources"):\n        return EmitError.MISSING_RESOURCES\n        \n    return None\n\n# Mock function to emit a signal (would use DB in real implementation)\ndef create_signal(signal_type: str, payload: Dict[str, Any], target_agent_id: Optional[str] = None) -> Dict[str, Any]:\n    """Create a signal in the database - mocked for demo"""\n    # In a real implementation, this would INSERT into the signals table\n    signal_id = str(uuid.uuid4())\n    \n    # Log what would happen in a real system\n    print(f"[DEMO] Creating {signal_type} signal {signal_id} with payload size {len(json.dumps(payload))}\\n")\n    \n    # Return signal creation result\n    return {\n        "id": signal_id,\n        "type": signal_type,\n        "created_at": datetime.now().isoformat(),\n        "target_agent_id": target_agent_id\n    }\n\n# Extract resource references from a persistence result\ndef extract_resources(persistence_result: Dict[str, Any]) -> List[ResourceReference]:\n    """Extract resource references from persistence results"""\n    resources = []\n    \n    # Process patient resource if present\n    patient_data = persistence_result.get("resources", {}).get("patient")\n    if patient_data:\n        patient_ref = create_resource_reference(patient_data)\n        if patient_ref:\n            resources.append(patient_ref)\n    \n    # Process encounter resource if present\n    encounter_data = persistence_result.get("resources", {}).get("encounter")\n    if encounter_data:\n        encounter_ref = create_resource_reference(encounter_data)\n        if encounter_ref:\n            resources.append(encounter_ref)\n    \n    return resources\n\n# Main function to emit completion signal\ndef emit_completion_signal(persistence_result: Dict[str, Any], emitter: Callable = create_signal) -> Result:\n    """Emit a completion signal with stored FHIR resource references"""\n    # Extract successfully stored resources\n    resources = extract_resources(persistence_result)\n    \n    # Create payload for FYI signal\n    payload = {\n        "status": "completed",\n        "resource_count": len(resources),\n        "resources": [asdict(r) for r in resources],\n        "process_id": str(uuid.uuid4()),  # Normally from runtime session\n        "timestamp": datetime.now().isoformat()\n    }\n    \n    # Validate payload\n    validation_error = validate_payload(payload)\n    if validation_error:\n        return validation_error\n    \n    # Emit the signal using the provided emitter function\n    try:\n        signal_result = emitter("fyi", payload=payload, target_agent_id=None)\n        \n        # Return successful result\n        return EmitResult(\n            signal_id=signal_result["id"],\n            timestamp=signal_result["created_at"],\n            resource_count=len(resources),\n            resources=resources\n        )\n    except Exception as e:\n        # Handle any errors in signal creation\n        return EmitError.SIGNAL_CREATION_FAILED\n\n# Process the results and emit a completion signal\ndef process():\n    # Get persistence results from previous step\n    persistence_result = source.get("data", {})\n    \n    if not persistence_result or persistence_result.get("status") == "error":\n        return {\n            "status": "error",\n            "message": "No valid persistence results to process",\n            "error": "MISSING_DATA"\n        }\n    \n    # Emit the completion signal\n    emit_result = emit_completion_signal(persistence_result)\n    \n    # Handle result types\n    if isinstance(emit_result, EmitError):\n        return {\n            "status": "error",\n            "message": f"Failed to emit completion signal: {emit_result.name}",\n            "error": emit_result.name\n        }\n    else:\n        # Convert dataclass to dict for JSON serialization\n        return {\n            "status": "success",\n            "message": f"Emitted completion signal for {emit_result.resource_count} resources",\n            "signal": asdict(emit_result)\n        }\n\n# Execute the process\nresult = process()'),
  -- Daily Topic Research Agent steps
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), 'Fetch News', 'Fetches recent articles about HL7 FHIR Consent', 'python', E'from typing import Dict, Any, List, Optional, Union, TypedDict\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime, timedelta\nfrom enum import Enum, auto\nfrom functools import reduce\nimport json\nimport hashlib\n\n# Type definitions for functional design\nclass FetchError(Enum):\n    CONNECTION_FAILED = auto()\n    AUTHENTICATION_FAILED = auto()\n    NO_RESULTS = auto()\n    PARSING_ERROR = auto()\n\n# Result type pattern\nResult = Union[List[Dict[str, Any]], FetchError]\n\n@dataclass(frozen=True)\nclass NewsArticle:\n    """Immutable news article container"""\n    title: str\n    source: str\n    url: str\n    published_date: str\n    summary: str = ""\n    topics: List[str] = field(default_factory=list)\n    \n    @property\n    def article_id(self) -> str:\n        """Generate a deterministic ID from article properties"""\n        content = f"{self.title}|{self.url}|{self.published_date}"\n        return hashlib.md5(content.encode()).hexdigest()\n\n# Mock data source for demonstration\ndef get_mock_articles() -> List[NewsArticle]:\n    """Return mock FHIR Consent news articles for demonstration"""\n    # Generate dates for the last few days\n    today = datetime.now()\n    yesterday = (today - timedelta(days=1)).strftime("%Y-%m-%d")\n    two_days_ago = (today - timedelta(days=2)).strftime("%Y-%m-%d")\n    \n    return [\n        NewsArticle(\n            title="FHIR Consent Resource Update in R5",\n            source="HL7.org",\n            url="https://hl7.org/fhir/updates/r5-consent",\n            published_date=yesterday,\n            topics=["FHIR R5", "Consent", "Privacy"]\n        ),\n        NewsArticle(\n            title="New Approach to Patient Consent in Healthcare Exchanges",\n            source="Journal of Health Informatics",\n            url="https://example.com/jhi/article12345",\n            published_date=yesterday,\n            topics=["Interoperability", "HIE", "Consent"]\n        ),\n        NewsArticle(\n            title="CMS Updates Regulatory Guidance on Digital Consent",\n            source="CMS.gov",\n            url="https://example.com/cms/updates/2025/digital-consent",\n            published_date=two_days_ago,\n            topics=["Regulatory", "CMS", "Digital Health"]\n        )\n    ]\n\n# Pure function to filter articles by relevance to a topic\ndef filter_by_topic(articles: List[NewsArticle], topic: str) -> List[NewsArticle]:\n    """Filter articles by relevance to a topic"""\n    # Case-insensitive search in topics and title\n    topic_lower = topic.lower()\n    \n    return [\n        article for article in articles\n        if topic_lower in [t.lower() for t in article.topics] or\n           topic_lower in article.title.lower()\n    ]\n\n# Mock fetch function to simulate API call\ndef fetch_news_articles(topic: str) -> Result:\n    """Fetch news articles on a specific topic"""\n    try:\n        # In a real implementation, this would make an API call\n        # to a news source or RSS feed\n        \n        # Get our mock data for demonstration\n        all_articles = get_mock_articles()\n        \n        # Filter by topic\n        topic_articles = filter_by_topic(all_articles, topic)\n        \n        if not topic_articles:\n            return FetchError.NO_RESULTS\n            \n        # Convert to serializable dictionaries\n        return [asdict(article) for article in topic_articles]\n    \n    except Exception as e:\n        print(f"Error fetching articles: {str(e)}\\n")\n        return FetchError.CONNECTION_FAILED\n\n# Process function to fetch and process articles\ndef process_topic_research():\n    # The topic we want to research (could come from signal data)\n    research_topic = "HL7 FHIR Consent"\n    \n    # Fetch the articles\n    fetch_result = fetch_news_articles(research_topic)\n    \n    # Handle errors or return the data\n    if isinstance(fetch_result, FetchError):\n        return {\n            "status": "error",\n            "error": fetch_result.name,\n            "message": f"Failed to fetch articles on {research_topic}: {fetch_result.name}"\n        }\n    \n    # Add metadata\n    result = {\n        "status": "success",\n        "topic": research_topic,\n        "article_count": len(fetch_result),\n        "fetched_at": datetime.now().isoformat(),\n        "articles": fetch_result\n    }\n    \n    return result\n\n# Execute the processing function\nresult = process_topic_research()'),
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), 'Summarise Articles', 'LLM summarisation of fetched articles', 'prompt', 'Summarise the following articles into a concise daily digest for compliance officers.'),
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), 'Store Summary', 'Stores the daily digest to the database', 'python', E'from typing import Dict, Any, List, Optional, Union, Protocol, TypeVar\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nfrom functools import partial, reduce\nimport json\nimport uuid\n\n# Type definitions for functional design\nT = TypeVar("T")\n\nclass StoreError(Enum):\n    CONNECTION_FAILED = auto()\n    INVALID_CONTENT = auto()\n    DUPLICATE_ENTRY = auto()\n    TRANSACTION_FAILED = auto()\n\n# Result type pattern\nResult = Union[Dict[str, Any], StoreError]\n\n@dataclass(frozen=True)\nclass DigestMetadata:\n    """Immutable digest metadata container"""\n    digest_id: str\n    topic: str\n    created_at: str\n    article_count: int\n    format: str = "markdown"\n\n@dataclass(frozen=True)\nclass Digest:\n    """Immutable digest container"""\n    metadata: DigestMetadata\n    content: str\n    tags: List[str] = field(default_factory=list)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        """Convert to serializable dictionary"""\n        return {\n            **asdict(self.metadata),\n            "content": self.content,\n            "tags": self.tags\n        }\n\n# Pure function to generate a unique ID\ndef generate_id(prefix: str = "digest") -> str:\n    """Generate a unique ID for the digest"""\n    return f"{prefix}_{uuid.uuid4().hex[:8]}"\n\n# Validate digest content\ndef validate_digest(content: str, article_count: int) -> Optional[str]:\n    """Validate digest content, returns error message or None"""\n    if not content or len(content.strip()) < 10:\n        return "Digest content is too short or empty"\n        \n    if article_count <= 0:\n        return "No articles were included in this digest"\n    \n    return None\n\n# Extract tags from digest content\ndef extract_tags(content: str) -> List[str]:\n    """Extract relevant tags from digest content"""\n    # Simple tag extraction based on common phrases\n    # In a real implementation, this might use NLP techniques\n    potential_tags = [\n        "FHIR", "Consent", "Privacy", "Regulatory", "CMS", \n        "Interoperability", "HL7", "R5", "HIE", "Standards"\n    ]\n    \n    return [tag for tag in potential_tags if tag.lower() in content.lower()]\n\n# Store digest to database (mock implementation)\ndef store_digest_to_db(digest: Digest) -> Result:\n    """Store a digest to the database and return a Result"""\n    try:\n        # In a real implementation, this would insert into a database\n        # For demo purposes, we just log what would happen\n        digest_dict = digest.to_dict()\n        print(f"[DEMO] Storing digest {digest.metadata.digest_id} with {digest.metadata.article_count} articles\\n")\n        \n        # Return success result\n        return {\n            "status": "success",\n            "digest_id": digest.metadata.digest_id,\n            "stored_at": datetime.now().isoformat(),\n            "topic": digest.metadata.topic,\n            "tag_count": len(digest.tags)\n        }\n    except Exception as e:\n        print(f"Error storing digest: {str(e)}\\n")\n        return StoreError.CONNECTION_FAILED\n\n# Main processing function\ndef process_summary():\n    # Get the summary response from previous step\n    summary_content = source.get("response", "")\n    \n    # Get topic and article info from first step\n    article_data = source.get("data", {})\n    topic = article_data.get("topic", "HL7 FHIR Consent")\n    article_count = article_data.get("article_count", 0)\n    \n    # Validate summary content\n    validation_error = validate_digest(summary_content, article_count)\n    if validation_error:\n        return {\n            "status": "error",\n            "message": f"Invalid digest: {validation_error}",\n            "error": "INVALID_CONTENT"\n        }\n    \n    # Extract tags\n    tags = extract_tags(summary_content)\n    \n    # Create digest metadata\n    metadata = DigestMetadata(\n        digest_id=generate_id(),\n        topic=topic,\n        created_at=datetime.now().isoformat(),\n        article_count=article_count\n    )\n    \n    # Create the digest\n    digest = Digest(\n        metadata=metadata,\n        content=summary_content,\n        tags=tags\n    )\n    \n    # Store digest\n    store_result = store_digest_to_db(digest)\n    \n    # Handle result types\n    if isinstance(store_result, StoreError):\n        return {\n            "status": "error",\n            "message": f"Failed to store digest: {store_result.name}",\n            "error": store_result.name\n        }\n    else:\n        return {\n            "status": "success",\n            "message": f"Stored digest on {topic} with {len(tags)} tags",\n            "digest": store_result\n        }\n\n# Execute the processing function\nresult = process_summary()'),
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), 'Notify Team', 'Emits a signal indicating the digest is ready', 'python', E'from typing import Dict, Any, List, Optional, Union, Protocol, TypeVar, Callable\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nfrom functools import partial\nimport uuid\nimport json\n\n# Type definitions for functional design\nT = TypeVar("T")\n\nclass NotifyError(Enum):\n    SIGNAL_CREATION_FAILED = auto()\n    MISSING_DIGEST = auto()\n    INVALID_PAYLOAD = auto()\n    NOTIFICATION_FAILURE = auto()\n\n# Result type pattern for error handling\nResult = Union[Dict[str, Any], NotifyError]\n\n# Protocol for dependency injection of signal creation\nclass SignalCreator(Protocol):\n    def __call__(self, signal_type: str, payload: Dict[str, Any], target_agent_id: Optional[str] = None) -> Dict[str, Any]: ...\n\n@dataclass(frozen=True)\nclass NotificationChannel:\n    """Immutable notification channel configuration"""\n    channel_id: str\n    channel_type: str  # email, slack, teams, signal, etc.\n    recipients: List[str]\n    priority: str = "normal"  # low, normal, high, urgent\n\n@dataclass(frozen=True)\nclass NotificationResult:\n    """Immutable notification result container"""\n    signal_id: str\n    timestamp: str\n    digest_id: str\n    channel_count: int\n    channel_ids: List[str] = field(default_factory=list)\n\n# Pure function to generate notification channels\ndef get_notification_channels(topic: str) -> List[NotificationChannel]:\n    """Return notification channels based on the topic"""\n    # In a real implementation, this would look up configured channels in a database\n    # For demo purposes, we return mock channels\n    \n    # For healthcare regulatory topics, we notify compliance and leadership\n    if any(term in topic.lower() for term in ["regulatory", "compliance", "privacy", "consent"]):\n        return [\n            NotificationChannel(\n                channel_id="email_compliance",\n                channel_type="email",\n                recipients=["compliance@example.org"],\n                priority="high"\n            ),\n            NotificationChannel(\n                channel_id="slack_leadership",\n                channel_type="slack",\n                recipients=["#leadership-updates"],\n                priority="normal"\n            )\n        ]\n    \n    # Default notification channel\n    return [\n        NotificationChannel(\n            channel_id="email_default",\n            channel_type="email",\n            recipients=["team@example.org"],\n            priority="normal"\n        )\n    ]\n\n# Mock signal creation (would use DB in real implementation)\ndef create_signal(signal_type: str, payload: Dict[str, Any], target_agent_id: Optional[str] = None) -> Dict[str, Any]:\n    """Create a signal in the database - mocked for demonstration"""\n    # In a real implementation, this would INSERT into the signals table\n    signal_id = str(uuid.uuid4())\n    \n    # Log what would happen in a real implementation\n    print(f"[DEMO] Creating {signal_type} signal {signal_id} with payload size {len(json.dumps(payload))}\\n")\n    \n    # Return signal creation result\n    return {\n        "id": signal_id,\n        "type": signal_type,\n        "created_at": datetime.now().isoformat(),\n        "target_agent_id": target_agent_id\n    }\n\n# Create digest summary for notification\ndef create_digest_summary(digest_data: Dict[str, Any]) -> str:\n    """Create a summary of the digest for notifications"""\n    digest_id = digest_data.get("digest_id", "unknown")\n    topic = digest_data.get("topic", "unknown")\n    tag_count = digest_data.get("tag_count", 0)\n    stored_at = digest_data.get("stored_at", datetime.now().isoformat())\n    \n    # Format a simple summary\n    return f"Daily digest on {topic} is ready. Contains {tag_count} key topics. ID: {digest_id}"\n\n# Emit FYI signal with digest data\ndef emit_digest_ready_signal(digest_data: Dict[str, Any], emitter: Callable = create_signal) -> Result:\n    """Emit an FYI signal indicating a digest is ready"""\n    # Validate we have a digest ID\n    digest_id = digest_data.get("digest_id")\n    if not digest_id:\n        return NotifyError.MISSING_DIGEST\n        \n    # Create payload for FYI signal\n    payload = {\n        "type": "digest_ready",\n        "digest_id": digest_id,\n        "topic": digest_data.get("topic", "unknown"),\n        "summary": create_digest_summary(digest_data),\n        "timestamp": datetime.now().isoformat()\n    }\n    \n    # Emit the signal using the provided emitter function\n    try:\n        signal_result = emitter("fyi", payload=payload)\n        \n        # Get notification channels for this topic\n        channels = get_notification_channels(payload.get("topic", ""))\n        \n        # Return successful result\n        return NotificationResult(\n            signal_id=signal_result["id"],\n            timestamp=signal_result["created_at"],\n            digest_id=digest_id,\n            channel_count=len(channels),\n            channel_ids=[ch.channel_id for ch in channels]\n        )\n    except Exception as e:\n        print(f"Error creating signal: {str(e)}\\n")\n        return NotifyError.SIGNAL_CREATION_FAILED\n\n# Main process function\ndef process_notification():\n    # Get digest data from previous step\n    digest_result = source.get("data", {})\n    \n    # Check if we have a digest to notify about\n    digest_data = digest_result.get("digest")\n    if not digest_data:\n        return {\n            "status": "error",\n            "message": "No digest data available to notify about",\n            "error": "MISSING_DIGEST"\n        }\n    \n    # Emit the notification signal\n    notification_result = emit_digest_ready_signal(digest_data)\n    \n    # Handle result types\n    if isinstance(notification_result, NotifyError):\n        return {\n            "status": "error",\n            "message": f"Failed to emit notification: {notification_result.name}",\n            "error": notification_result.name\n        }\n    else:\n        # Convert dataclass to dict for JSON serialization\n        return {\n            "status": "success",\n            "message": f"Notified {notification_result.channel_count} channels about digest {notification_result.digest_id}",\n            "notification": asdict(notification_result)\n        }\n\n# Execute the process\nresult = process_notification()'),
  -- Coordinator Agent steps
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), 'Init Inquiry', 'Generates clarifying request text', 'prompt', 'The incoming request is: [question]. Ask any clarifications needed.'),
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), 'Send Signal To Specialist', 'Creates a specialist_inquiry signal for the Specialist Agent', 'python', E'from typing import Dict, Any, List, Optional, Union, Protocol, TypeVar\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nimport json\nimport uuid\n\n# Type definitions for functional design\nT = TypeVar("T")\n\nclass EmitError(Enum):\n    SIGNAL_CREATION_FAILED = auto()\n    INVALID_QUESTION = auto()\n    MISSING_TARGET = auto()\n\n# Result type pattern for error handling\nResult = Union[Dict[str, Any], EmitError]\n\n@dataclass(frozen=True)\nclass InquiryPayload:\n    """Immutable inquiry payload container"""\n    question_id: str\n    question: str\n    clarification: str\n    parent_signal_id: Optional[str] = None\n    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n    \n    def to_dict(self) -> Dict[str, Any]:\n        """Convert to dictionary for serialization"""\n        result = {\n            "question_id": self.question_id,\n            "question": self.question,\n            "clarification": self.clarification,\n            "created_at": self.created_at\n        }\n        \n        if self.parent_signal_id:\n            result["parent_signal_id"] = self.parent_signal_id\n            \n        return result\n\n# Mock function to get specialist agent ID\ndef get_specialist_agent_id() -> str:\n    """Get the ID of the specialist agent"""\n    # In a real implementation, this would look up the agent ID from the database\n    # For demo purposes, we use a query to get it dynamically\n    return "(SELECT id FROM agents WHERE agent_name=\'specialist_agent\')"\n\n# Mock signal creation (would use DB in real implementation)\ndef create_signal(agent_id: str, signal_type: str, initial_data: Dict[str, Any]) -> Dict[str, Any]:\n    """Create a signal in the database - mocked for demonstration"""\n    # In a real implementation, this would INSERT into the signals table\n    signal_id = str(uuid.uuid4())\n    \n    # Log what would happen in a real implementation\n    print(f"[DEMO] Creating {signal_type} signal {signal_id} for agent {agent_id} with payload size {len(json.dumps(initial_data))}\\n")\n    \n    # Return signal creation result\n    return {\n        "id": signal_id,\n        "type": signal_type,\n        "agent_id": agent_id,\n        "created_at": datetime.now().isoformat(),\n        "initial_data": initial_data\n    }\n\n# Create and emit a specialist inquiry signal\ndef emit_specialist_inquiry(clarification: str, original_question: Optional[str] = None) -> Result:\n    """Emit a specialist inquiry signal with clarification text"""\n    # Get the question from source or parameter\n    question = original_question\n    \n    # If not provided, try to get from source\n    if not question:\n        source_data = source.get("data", {})\n        question = source_data.get("question", "")\n        \n    # Validate we have a question\n    if not question or len(question.strip()) < 3:\n        return EmitError.INVALID_QUESTION\n    \n    # Get the parent signal ID if available\n    parent_id = source.get("signal_id", None)\n    \n    # Create the inquiry payload\n    payload = InquiryPayload(\n        question_id=str(uuid.uuid4().hex[:8]),\n        question=question,\n        clarification=clarification,\n        parent_signal_id=parent_id\n    )\n    \n    # Get the specialist agent ID\n    specialist_id = get_specialist_agent_id()\n    \n    # Create the signal\n    try:\n        signal_result = create_signal(\n            agent_id=specialist_id,\n            signal_type="run",\n            initial_data=payload.to_dict()\n        )\n        \n        return {\n            "status": "success",\n            "message": f"Created specialist inquiry signal for question: {question[:30]}...",\n            "signal_id": signal_result["id"],\n            "target_agent_id": specialist_id,\n            "created_at": signal_result["created_at"]\n        }\n    except Exception as e:\n        print(f"Error creating signal: {str(e)}\\n")\n        return EmitError.SIGNAL_CREATION_FAILED\n\n# Main process function\ndef process():\n    # Get the clarification text from previous step\n    clarification = source.get("response", "")\n    \n    if not clarification:\n        return {\n            "status": "error",\n            "message": "No clarification text available from previous step",\n            "error": "MISSING_CLARIFICATION"\n        }\n    \n    # Emit the specialist inquiry signal\n    result = emit_specialist_inquiry(clarification)\n    \n    # Handle result types\n    if isinstance(result, EmitError):\n        return {\n            "status": "error",\n            "message": f"Failed to emit specialist inquiry: {result.name}",\n            "error": result.name\n        }\n    else:\n        return result\n\n# Execute the process\nresult = process()'),
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), 'Await Response', 'Waits for a specialist_response signal', 'python', E'from typing import Dict, Any, List, Optional, Union, Callable\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime, timedelta\nfrom enum import Enum, auto\nimport json\nimport time\nimport uuid\n\n# Type definitions for functional design\nclass AwaitError(Enum):\n    TIMEOUT = auto()\n    NO_PARENT_SIGNAL = auto()\n    QUERY_ERROR = auto()\n\n# Result type pattern for error handling\nResult = Union[Dict[str, Any], AwaitError]\n\n@dataclass(frozen=True)\nclass ResponseData:\n    """Immutable response data container"""\n    signal_id: str\n    response_text: str\n    created_at: str\n    mapping_suggestions: List[Dict[str, str]] = field(default_factory=list)\n    reference_ids: List[str] = field(default_factory=list)\n\n# Mock query function to check for specialist responses\ndef query_specialist_responses(parent_signal_id: str) -> List[Dict[str, Any]]:\n    """Query for specialist response signals referencing parent signal"""\n    # In a real implementation, this would query the database\n    # For demo purposes, we simulate finding a response after a short delay\n    \n    # Simulate some processing time\n    time.sleep(0.1)\n    \n    # For demonstration purposes, always return a mock response\n    return [{\n        "id": str(uuid.uuid4()),\n        "signal_type": "specialist_response",\n        "created_at": datetime.now().isoformat(),\n        "data": {\n            "parent_signal_id": parent_signal_id,\n            "response": "The CPT code 99214 maps to ICD-10 code E11.9 for Type 2 diabetes without complications.",\n            "mapping_suggestions": [\n                {"cpt": "99214", "icd10": "E11.9", "description": "Type 2 diabetes without complications"}\n            ],\n            "reference_ids": ["ref_123", "ref_456"]\n        }\n    }]\n\n# Function to wait for specialist response with timeout\ndef await_response(parent_signal_id: Optional[str] = None, timeout_seconds: int = 10, poll_interval: float = 0.5) -> Result:\n    """Wait for a specialist response signal, with timeout"""\n    # Get parent signal ID if not provided\n    if not parent_signal_id:\n        # Try to get from previous step result\n        prev_result = source.get("data", {})\n        parent_signal_id = prev_result.get("signal_id")\n        \n        # If still not found, try source properties\n        if not parent_signal_id:\n            parent_signal_id = source.get("signal_id")\n    \n    # Validate we have a parent signal ID\n    if not parent_signal_id:\n        return AwaitError.NO_PARENT_SIGNAL\n    \n    # Calculate timeout deadline\n    deadline = datetime.now() + timedelta(seconds=timeout_seconds)\n    \n    # Poll for responses until timeout\n    try:\n        while datetime.now() < deadline:\n            # Query for responses\n            responses = query_specialist_responses(parent_signal_id)\n            \n            # If we found responses, process them\n            if responses:\n                # Get the newest response\n                newest_response = responses[0]  # In real code, we would sort by date\n                \n                # Extract response data\n                response_data = newest_response.get("data", {})\n                \n                # Create response object\n                return ResponseData(\n                    signal_id=newest_response["id"],\n                    response_text=response_data.get("response", ""),\n                    created_at=newest_response["created_at"],\n                    mapping_suggestions=response_data.get("mapping_suggestions", []),\n                    reference_ids=response_data.get("reference_ids", [])\n                )\n            \n            # Wait before polling again\n            time.sleep(poll_interval)\n        \n        # If we got here, we timed out\n        return AwaitError.TIMEOUT\n    \n    except Exception as e:\n        print(f"Error querying responses: {str(e)}\\n")\n        return AwaitError.QUERY_ERROR\n\n# Main process function\ndef process():\n    # Get the parent signal ID from previous step\n    prev_result = source.get("data", {})\n    parent_signal_id = prev_result.get("signal_id")\n    \n    # Await the specialist response\n    await_result = await_response(parent_signal_id, timeout_seconds=5)\n    \n    # Handle result types\n    if isinstance(await_result, AwaitError):\n        return {\n            "status": "error",\n            "message": f"Failed to get specialist response: {await_result.name}",\n            "error": await_result.name\n        }\n    else:\n        # Convert dataclass to dict for JSON serialization\n        return {\n            "status": "success",\n            "message": "Received specialist response",\n            "response": asdict(await_result)\n        }\n\n# Execute the process\nresult = process()'),
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), 'Decide Conclusion', 'Determines if the specialist response resolves the request', 'prompt', 'Decide if the specialist response resolves the inquiry. Reply with complete or continue.'),
  -- Specialist Agent steps
  ((SELECT id FROM agents WHERE agent_name='specialist_agent'), 'Analyse Request', 'Provides ICD-10 mapping suggestions', 'prompt', 'You are a coding specialist. Provide ICD-10 mappings for the supplied CPT code.'),
  ((SELECT id FROM agents WHERE agent_name='specialist_agent'), 'Reply To Coordinator', 'Emits a specialist_response signal', 'python', E'from typing import Dict, Any, List, Optional, Union, TypedDict, Protocol\nfrom dataclasses import dataclass, asdict, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nimport json\nimport uuid\n\n# Type definitions for functional design\nclass ReplyError(Enum):\n    SIGNAL_CREATION_FAILED = auto()\n    MISSING_PARENT = auto()\n    INVALID_MAPPING = auto()\n\n# Result type pattern for error handling\nResult = Union[Dict[str, Any], ReplyError]\n\n@dataclass(frozen=True)\nclass MappingSuggestion:\n    """Immutable mapping suggestion"""\n    cpt: str\n    icd10: str\n    description: str\n\n@dataclass(frozen=True)\nclass ResponsePayload:\n    """Immutable response payload"""\n    parent_signal_id: str\n    response: str\n    mapping_suggestions: List[MappingSuggestion] = field(default_factory=list)\n    reference_ids: List[str] = field(default_factory=list)\n    created_at: str = field(default_factory=lambda: datetime.now().isoformat())\n    \n    def to_dict(self) -> Dict[str, Any]:\n        """Convert to dictionary for serialization"""\n        return {\n            "parent_signal_id": self.parent_signal_id,\n            "response": self.response,\n            "mapping_suggestions": [asdict(ms) for ms in self.mapping_suggestions],\n            "reference_ids": self.reference_ids,\n            "created_at": self.created_at\n        }\n\n# Create mapping suggestions from LLM response\ndef parse_mapping_suggestions(response_text: str) -> List[MappingSuggestion]:\n    """Parse mapping suggestions from the response text"""\n    # In a real implementation, this would parse structured data\n    # For demo purposes, we return hard-coded suggestions\n    \n    # Simple keyword-based parsing\n    if "diabetes" in response_text.lower():\n        return [\n            MappingSuggestion(\n                cpt="99214",\n                icd10="E11.9",\n                description="Type 2 diabetes without complications"\n            )\n        ]\n    elif "hypertension" in response_text.lower():\n        return [\n            MappingSuggestion(\n                cpt="99213",\n                icd10="I10",\n                description="Essential (primary) hypertension"\n            )\n        ]\n    else:\n        # Default mapping suggestion\n        return [\n            MappingSuggestion(\n                cpt="99211",\n                icd10="Z00.00",\n                description="Encounter for general adult medical examination without abnormal findings"\n            )\n        ]\n\n# Mock coordinator agent ID lookup\ndef get_coordinator_agent_id() -> str:\n    """Get the ID of the coordinator agent"""\n    # In a real implementation, this would look up the agent ID from the database\n    # For demo purposes, we use a query to get it dynamically\n    return "(SELECT id FROM agents WHERE agent_name=\'coordinator_agent\')"\n\n# Create and emit a response signal\ndef create_response_signal(payload: ResponsePayload) -> Result:\n    """Create a signal in the database with the response payload"""\n    try:\n        # In a real implementation, this would INSERT into the signals table\n        signal_id = str(uuid.uuid4())\n        \n        # Get the coordinator agent ID\n        coordinator_id = get_coordinator_agent_id()\n        \n        # Log what would happen in a real implementation\n        print(f"[DEMO] Creating specialist_response signal {signal_id} with payload size {len(json.dumps(payload.to_dict()))}\\n")\n        \n        # Return success result\n        return {\n            "status": "success",\n            "signal_id": signal_id,\n            "agent_id": coordinator_id,\n            "signal_type": "specialist_response",\n            "created_at": datetime.now().isoformat()\n        }\n    except Exception as e:\n        print(f"Error creating signal: {str(e)}\\n")\n        return ReplyError.SIGNAL_CREATION_FAILED\n\n# Main process function\ndef process():\n    # Get the analysis response from previous step\n    response_text = source.get("response", "")\n    \n    # Get the original inquiry data\n    inquiry_data = source.get("data", {})\n    parent_signal_id = inquiry_data.get("parent_signal_id")\n    original_question = inquiry_data.get("question", "")\n    \n    # Validate we have a parent signal ID\n    if not parent_signal_id:\n        return {\n            "status": "error",\n            "message": "Missing parent signal ID",\n            "error": "MISSING_PARENT"\n        }\n    \n    # Parse mapping suggestions\n    mapping_suggestions = parse_mapping_suggestions(response_text)\n    \n    # Create response payload\n    payload = ResponsePayload(\n        parent_signal_id=parent_signal_id,\n        response=response_text,\n        mapping_suggestions=mapping_suggestions,\n        reference_ids=[f"ref_{uuid.uuid4().hex[:6]}" for _ in range(2)]  # Mock reference IDs\n    )\n    \n    # Create and emit the signal\n    result = create_response_signal(payload)\n    \n    # Handle result types\n    if isinstance(result, ReplyError):\n        return {\n            "status": "error",\n            "message": f"Failed to create response signal: {result.name}",\n            "error": result.name\n        }\n    else:\n        return {\n            "status": "success",\n            "message": f"Sent mapping response for question: {original_question[:30]}...",\n            "signal": {\n                "id": result["signal_id"],\n                "created_at": result["created_at"],\n                "mapping_count": len(mapping_suggestions)\n            }\n        }\n\n# Execute the process\nresult = process()');

-- Update agent step_ids array
UPDATE agents
SET step_ids = (SELECT ARRAY_AGG(id ORDER BY id) FROM steps WHERE agent_id = agents.id)
WHERE agent_name IN ('hacker_news_scraper', 'hl7_to_fhir_agent', 'topic_research_agent', 'coordinator_agent', 'specialist_agent');

-- Insert sample Signals with completed status
-- Demo Signals to showcase workflows
INSERT INTO signals (agent_id, user_requested_uuid, signal_type, initial_data) VALUES
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), gen_random_uuid(), 'run', '{"raw_hl7": "MSH|^~&|ADT"}'),
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), gen_random_uuid(), 'run', '{}'),
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), gen_random_uuid(), 'run', '{"question": "Need ICD-10 mapping for new CPT code"}'),
  ((SELECT id FROM agents WHERE agent_name='pacs_integration_agent'), gen_random_uuid(), 'run', '{"dicom_file": "MR54321.dcm"}');

-- NOTE: RuntimeSession data is now generated by the Python script at:
-- server/examples/scripts/generate_seed_data.py
-- This allows for more flexible and comprehensive test data generation
-- Run: cd server/examples && python scripts/generate_seed_data.py
-- HL7 to FHIR Converter RuntimeSession
(
  gen_random_uuid(),
  NOW() - INTERVAL '3 hours',
  NOW() - INTERVAL '2 hours 59 minutes',
  (SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'),
  'completed',
  '{"raw_hl7": "MSH|^~&|ADT"}',
  3,
  '{"status":"success","message":"Converted ADT message to FHIR resources","resources":{"patient":{"resourceType":"Patient","id":"pat1"},"encounter":{"resourceType":"Encounter","id":"enc1"}}}',
  ARRAY[2.341, 8.723, 1.845, 1.341],
  (SELECT ARRAY_AGG(id) FROM steps WHERE agent_id = (SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent')),
  14.25,
  ARRAY['{
    "status":"success",
    "message":"Successfully parsed HL7 message",
    "data":{"MSH":{"sending_app":"SENDING","message_type":"ADT^A04"},"PID":{"patient_id":"12345","name":"DOE^JOHN"}}
  }'::json,
  '{
    "status":"success",
    "message":"Mapped to FHIR resources",
    "patient":{"resourceType":"Patient","id":"pat1","name":[{"family":"Doe","given":["John"]}]},
    "encounter":{"resourceType":"Encounter","id":"enc1","status":"finished"}
  }'::json,
  '{
    "status":"success",
    "message":"Processed 2 FHIR resources",
    "resources":{"patient":{"status":"success","resource_id":"pat1"},"encounter":{"status":"success","resource_id":"enc1"}}
  }'::json,
  '{
    "status":"success",
    "message":"Emitted completion signal for 2 resources",
    "signal":{"signal_id":"f7d8a12e-3b45-4c76-9f2a-8b5c12e34d56"}
  }'::json]
),


-- Topic Research RuntimeSession
(
  gen_random_uuid(),
  NOW() - INTERVAL '2 hours',
  NOW() - INTERVAL '1 hour 56 minutes',
  (SELECT id FROM agents WHERE agent_name='topic_research_agent'),
  'completed',
  '{"topic": "HL7 FHIR Consent"}',
  3,
  '{"status":"success","message":"Daily digest created and notifications sent","digest":{"digest_id":"digest_a7f3b921","topic":"HL7 FHIR Consent","tag_count":4}}',
  ARRAY[5.432, 224.561, 4.512, 5.495],
  (SELECT ARRAY_AGG(id) FROM steps WHERE agent_id = (SELECT id FROM agents WHERE agent_name='topic_research_agent')),
  240.0,
  ARRAY['{
    "status":"success",
    "topic":"HL7 FHIR Consent",
    "article_count":3,
    "message":"Found 3 articles on HL7 FHIR Consent"
  }'::json,
  '{
    "status":"success",
    "message":"Generated summary with 450 words"
  }'::json,
  '{
    "status":"success",
    "message":"Stored digest with 4 tags",
    "digest":{"digest_id":"digest_a7f3b921","topic":"HL7 FHIR Consent","tag_count":4,"stored_at":"2025-05-08T01:05:23-07:00"}
  }'::json,
  '{
    "status":"success",
    "message":"Notified 2 channels about digest",
    "notification":{"channel_count":2,"digest_id":"digest_a7f3b921"}
  }'::json]
),


-- Coordinator-Specialist Multi-Agent Communication
(
  gen_random_uuid(),
  NOW() - INTERVAL '1 hour',
  NOW() - INTERVAL '59 minutes',
  (SELECT id FROM agents WHERE agent_name='coordinator_agent'),
  'completed',
  '{"question": "Need ICD-10 mapping for new CPT code"}',
  3,
  '{"status":"success","message":"Request fulfilled: ICD-10 mapping provided","mapping":{"cpt":"99214","icd10":"E11.9","description":"Type 2 diabetes without complications"}}',
  ARRAY[12.456, 3.245, 45.678, 5.621],
  (SELECT ARRAY_AGG(id) FROM steps WHERE agent_id = (SELECT id FROM agents WHERE agent_name='coordinator_agent')),
  67.0,
  ARRAY['{
    "status":"success",
    "message":"Generated clarification question"
  }'::json,
  '{
    "status":"success",
    "message":"Created specialist inquiry signal",
    "signal_id":"a1b2c3d4-e5f6-4a5b-8c9d-0e1f2a3b4c5d",
    "target_agent_id":"(SELECT id FROM agents WHERE agent_name=''specialist_agent'')"
  }'::json,
  '{
    "status":"success",
    "message":"Received specialist response",
    "response":{"signal_id":"f7d8e9c6-5b4a-3c2d-1e0f-9a8b7c6d5e4f","response_text":"The CPT code 99214 maps to ICD-10 code E11.9 for Type 2 diabetes without complications.","mapping_suggestions":[{"cpt":"99214","icd10":"E11.9","description":"Type 2 diabetes without complications"}]}
  }'::json,
  '{
    "status":"success",
    "message":"Determined response resolves inquiry",
    "decision":"complete"
  }'::json]
),


-- The corresponding Specialist Agent RuntimeSession
(
  gen_random_uuid(),
  NOW() - INTERVAL '1 hour',
  NOW() - INTERVAL '59 minutes 30 seconds',
  (SELECT id FROM agents WHERE agent_name='specialist_agent'),
  'completed',
  '{"question": "Need ICD-10 mapping for CPT code 99214", "clarification": "Please provide the most specific ICD-10 code for diabetes management visit using CPT 99214"}',
  1,
  '{"status":"success","message":"Mapping provided for CPT code 99214","mapping_count":1}',
  ARRAY[25.432, 4.568],
  (SELECT ARRAY_AGG(id) FROM steps WHERE agent_id = (SELECT id FROM agents WHERE agent_name='specialist_agent')),
  30.0,
  ARRAY['{
    "status":"success",
    "message":"Generated ICD-10 mapping for CPT 99214"
  }'::json,
  '{
    "status":"success",
    "message":"Sent mapping response with 1 suggestion",
    "signal":{"id":"f7d8e9c6-5b4a-3c2d-1e0f-9a8b7c6d5e4f","mapping_count":1}
  }'::json]
);

-- Confirm the seeded data
SELECT 'Agents: ' || COUNT(*) FROM agents;
SELECT 'Steps: ' || COUNT(*) FROM steps;
SELECT 'Signals: ' || COUNT(*) FROM signals;
SELECT 'Runtime Sessions: ' || COUNT(*) FROM runtime_sessions;

-- Add Supabase realtime tables
ALTER PUBLICATION supabase_realtime ADD TABLE signals;
ALTER PUBLICATION supabase_realtime ADD TABLE agents;

-- Demo Signals to showcase workflows
INSERT INTO signals (agent_id, user_requested_uuid, signal_type, initial_data) VALUES
  ((SELECT id FROM agents WHERE agent_name='hl7_to_fhir_agent'), gen_random_uuid(), 'run', '{"raw_hl7": "MSH|^~&|ADT"}'),
  ((SELECT id FROM agents WHERE agent_name='topic_research_agent'), gen_random_uuid(), 'run', '{}'),
  ((SELECT id FROM agents WHERE agent_name='coordinator_agent'), gen_random_uuid(), 'run', '{"question": "Need ICD-10 mapping for new CPT code"}');
